{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Packages installation"
      ],
      "metadata": {
        "id": "8p-veQ7hJupS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGpa_4Po0tC5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import torch\n",
        "\n",
        "# 1. Install Unsloth (Optimized for Colab)\n",
        "# We allow unsloth to handle the dependency resolution for Colab\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# 2. Install/Upgrade Xformers and TRL\n",
        "# We force these upgrades to ensure compatibility with the T4 GPU on Colab\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "# 3. Verify Pytorch & GPU\n",
        "print(f\"Pytorch Version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Please check Runtime settings.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Model (Mistral7B)"
      ],
      "metadata": {
        "id": "MvnnUPApJ0lP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "print(\"SUCCESS: Model loaded without errors!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X1wREqr1CSk",
        "outputId": "2d43468c-478f-4eb1-a404-772cd439ffae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2026.1.4: Fast Mistral patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "SUCCESS: Model loaded without errors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading and Formatting the Data"
      ],
      "metadata": {
        "id": "5mj_afpHJ-Xb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import os\n",
        "\n",
        "# 1. Define the specific path\n",
        "dataset_path = \"unsloth_training_data.json\"\n",
        "\n",
        "# 2. Verify the file exists before trying to load it\n",
        "if not os.path.exists(dataset_path):\n",
        "    raise FileNotFoundError(f\"File not found at: {dataset_path}\\nPlease upload 'unsloth_training_data.json' to the Colab Files tab on the left.\")\n",
        "\n",
        "print(f\"Found dataset at: {dataset_path}\")\n",
        "\n",
        "# 3. Load the JSON file\n",
        "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
        "\n",
        "# 4. Apply Mistral Chat Formatting\n",
        "# This requires 'tokenizer' to be defined from the previous Step 2 block\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"mistral\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "# Apply formatting to the entire dataset\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "print(\"SUCCESS: Data loaded and formatted!\")\n",
        "print(f\"Sample Input:\\n{dataset['text'][0][:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7xlHDjj1EM1",
        "outputId": "8896542a-b463-4304-a86f-772a8c223a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found dataset at: /content/unsloth_training_data.json\n",
            "SUCCESS: Data loaded and formatted!\n",
            "Sample Input:\n",
            "<s>[INST] Schema: {\n",
            "  \"db_id\": \"department_management\",\n",
            "  \"collection_names\": [\n",
            "    \"head\",\n",
            "    \"management\",\n",
            "    \"department\"\n",
            "  ],\n",
            "  \"column_names\": [\n",
            "    [\n",
            "      0,\n",
            "      \"_id\"\n",
            "    ],\n",
            "    [\n",
            "      0,...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the Hyperparameters for LoRA"
      ],
      "metadata": {
        "id": "lfnpnzigKB6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")\n",
        "\n",
        "print(\"LoRA Adapters attached successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvLQfLQa1GCb",
        "outputId": "d4a04dab-867c-4a43-afcb-890800e047cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2026.1.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Adapters attached successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Fine-Tuning"
      ],
      "metadata": {
        "id": "PGYOx8ruKaCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive (So we can save safely)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define a Safe Path on Drive\n",
        "# Checkpoints will be saved to your Drive, so they persist even if Colab crashes.\n",
        "safe_output_dir = \"/content/drive/MyDrive/DocSpider_Checkpoints\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "os.makedirs(safe_output_dir, exist_ok=True)\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"Starting FULL TRAINING...\")\n",
        "print(f\"Checkpoints will be saved to: {safe_output_dir}\")\n",
        "print(\"If the session crashes, you can resume from there!\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    processing_class = tokenizer,\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        # --- Memory Settings ---\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        gradient_checkpointing = True,\n",
        "\n",
        "        # --- Training Settings ---\n",
        "        num_train_epochs = 1,\n",
        "\n",
        "        # --- SAFE CHECKPOINTING ---\n",
        "        save_strategy = \"steps\",\n",
        "        save_steps = 100,            # Save every 100 steps (more frequent = safer)\n",
        "        output_dir = safe_output_dir, # <--- SAVES TO GOOGLE DRIVE\n",
        "        save_total_limit = 2,        # Only keep the last 2 checkpoints to save Drive space\n",
        "\n",
        "        # --- Optimizer ---\n",
        "        warmup_steps = 50,\n",
        "        learning_rate = 1e-4,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        seed = 3407,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# --- RESUME LOGIC ---\n",
        "# If you crash, change this to True. For the first run, keep it False.\n",
        "trainer_stats = trainer.train(resume_from_checkpoint = False)\n",
        "\n",
        "print(\"FULL TRAINING COMPLETE!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kqrGhCHoNVvx",
        "outputId": "69205d21-b5e9-430d-a692-dfd32e39c617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Starting FULL TRAINING...\n",
            "Checkpoints will be saved to: /content/drive/MyDrive/DocSpider_Checkpoints\n",
            "If the session crashes, you can resume from there!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 4,043 | Num Epochs = 1 | Total steps = 506\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 83,886,080 of 7,331,909,632 (1.14% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260201_075843-eef7x2it</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='458' max='506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [458/506 3:27:00 < 21:47, 0.04 it/s, Epoch 0.90/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.266000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.218100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.220800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.229100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.236900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.201400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.207300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.191100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.195000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.170800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.167100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.192300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.173200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.178000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.179400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.166300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.196600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.156600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.154300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.168800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.162100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.167300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.170200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.146600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.167700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.166400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.164700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.161500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.147500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.170900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.161100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.152200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.137900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.166200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.140300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.155300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.145300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.160300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.142700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.140500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: WARNING URL not available in offline run\n",
            "wandb: WARNING URL not available in offline run\n",
            "wandb: WARNING URL not available in offline run\n",
            "wandb: WARNING URL not available in offline run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "37PHQudMKe8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Mount Drive (to access your saved checkpoints)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define Paths\n",
        "checkpoint_folder = \"/content/drive/MyDrive/DocSpider_Checkpoints\"\n",
        "final_destination = \"/content/drive/MyDrive/DocSpider_Mistral_Final\"\n",
        "\n",
        "# 3. Find the latest checkpoint (e.g., checkpoint-500)\n",
        "if not os.path.exists(checkpoint_folder):\n",
        "    raise FileNotFoundError(\"Could not find checkpoints folder! Did you unmount Drive?\")\n",
        "\n",
        "# Get all folders starting with 'checkpoint-'\n",
        "checkpoints = [d for d in os.listdir(checkpoint_folder) if d.startswith(\"checkpoint-\")]\n",
        "\n",
        "if not checkpoints:\n",
        "    raise FileNotFoundError(\"No checkpoints found! Training might have failed before step 100.\")\n",
        "\n",
        "# Sort to find the highest number (latest step)\n",
        "# This logic sorts 'checkpoint-100', 'checkpoint-500' correctly\n",
        "latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[1]))\n",
        "latest_path = os.path.join(checkpoint_folder, latest_checkpoint)\n",
        "\n",
        "print(f\"âœ… Found latest saved model: {latest_checkpoint}\")\n",
        "\n",
        "# 4. Copy it to the Final Folder\n",
        "print(f\"Copying {latest_checkpoint} to {final_destination}...\")\n",
        "\n",
        "if os.path.exists(final_destination):\n",
        "    shutil.rmtree(final_destination) # Remove old version if exists\n",
        "\n",
        "shutil.copytree(latest_path, final_destination)\n",
        "\n",
        "print(\"SUCCESS: Your training is complete and saved!\")\n",
        "print(f\"You can now load your model from: {final_destination}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uuob6h3RQY-a",
        "outputId": "95625a4f-3d5c-435b-f1dd-318942e31ba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Found latest saved model: checkpoint-400\n",
            "Copying checkpoint-400 to /content/drive/MyDrive/DocSpider_Mistral_Final...\n",
            "SUCCESS: Your training is complete and saved!\n",
            "You can now load your model from: /content/drive/MyDrive/DocSpider_Mistral_Final\n"
          ]
        }
      ]
    }
  ]
}